# ============================================
# AI Medical Data Platform - Configuration
# ============================================
# Copy this file to .env and configure your settings
# All components read from this single file!
#
# DEFAULT SETUP: Ollama runs INSIDE Docker (no local install needed!)
# Just run: ./run.sh start

# ============================================
# 1. LLM PROVIDER SELECTION (Choose ONE)
# ============================================
# This is the ONLY place you need to set your model choice!
# Options:
#   Docker Ollama (default): ollama-mistral, ollama-llama3, ollama-meditron
#   API-based:               claude, openai, gemini
#   HuggingFace:             llama-3-8b, mistral-7b, meditron-7b

LLM_PROVIDER=ollama-mistral

# ============================================
# 2. API KEYS (Only needed for cloud API providers)
# ============================================

# Claude (Anthropic) - Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-key-here

# OpenAI (GPT-4) - Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-key-here

# Google Gemini - Get from: https://makersuite.google.com/app/apikey
# Both GEMINI_API_KEY and GOOGLE_API_KEY are supported
GEMINI_API_KEY=your-key-here
GOOGLE_API_KEY=your-key-here

# Hugging Face (Optional - for gated models like Llama)
HUGGINGFACE_TOKEN=hf_your-token-here

# ============================================
# 3. OLLAMA CONFIGURATION (Docker Service)
# ============================================
# Ollama runs as a Docker service automatically.
# Only change if you run Ollama externally.
OLLAMA_HOST=http://ollama:11434

# ============================================
# 4. LLM SETTINGS
# ============================================
USE_QUANTIZATION=true
LLM_DEVICE=auto
LLM_MAX_TOKENS=1024
LLM_TEMPERATURE=0.1

# ============================================
# 5. NEO4J DATABASE (Required)
# ============================================
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# ============================================
# 6. FILE PATHS
# ============================================
GUIDELINES_PATH=outputs/guidelines.json
CLINICAL_NOTES_PATH=outputs/clinical_notes.json
MODEL_CACHE_DIR=models/entity_extractor
PREDICTION_LOG_DIR=outputs/predictions

# ============================================
# 7. API SERVER
# ============================================
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_BASE_URL=http://localhost:8000

# ============================================
# 8. DASHBOARD
# ============================================
DASHBOARD_HOST=0.0.0.0
DASHBOARD_PORT=8050
DASHBOARD_DEBUG=false

# ============================================
# 9. LOGGING
# ============================================
LOG_LEVEL=INFO
LOG_FILE=outputs/system.log

# ============================================
# 10. GOOGLE CLOUD (Optional - for deployment)
# ============================================
GCP_PROJECT_ID=your-project-id
GCP_REGION=us-central1

# ============================================
# QUICK START EXAMPLES:
# ============================================
#
# Example 1: Docker Ollama + Mistral (default, free, private)
#   LLM_PROVIDER=ollama-mistral
#   Just run: ./run.sh start
#
# Example 2: Use Gemini (cloud, free tier)
#   LLM_PROVIDER=gemini
#   GEMINI_API_KEY=AIza...
#
# Example 3: Use Claude (best quality, paid)
#   LLM_PROVIDER=claude
#   ANTHROPIC_API_KEY=sk-ant-...
#
# Example 4: Use Meditron via Docker Ollama (medical-specific)
#   LLM_PROVIDER=ollama-meditron
#   (auto-pulled on first startup)
#
# ============================================
